{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BNExUNGSSau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sim_books(U):\n",
        "    from ast import literal_eval\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import warnings\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.decomposition import TruncatedSVD\n",
        "    from scipy.sparse import csr_matrix\n",
        "    import faiss\n",
        "    import random\n",
        "\n",
        "    warnings.filterwarnings(action='ignore')\n",
        "    books = pd.read_csv('/content/drive/MyDrive/nlbooks.csv', encoding='utf-8')\n",
        "    books = books.replace({np.nan: 'none'})\n",
        "\n",
        "    books_df=books[['isbn13','title','img_url','description']]\n",
        "\n",
        "    #books_df[['description']=='none']=None\n",
        "    books_df_nax=books_df.dropna(axis='rows')\n",
        "\n",
        "\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    # fit_transform안에 데이터프레임형태로 넣어주면 안됨. 하나의 변수씩만 넣어주자!\n",
        "    # title_vect1 = cnt_vect.fit_transform(books_df_nax['title'])\n",
        "    title_vect2 = tfidf.fit_transform(books_df_nax['title'])\n",
        "    #des_vect = cnt_vect.fit_transform(books['description'])\n",
        "    #books['book_info']=books['title']+books['description']\n",
        "    #b_category=books['kdc_class_no']\n",
        "\n",
        "    # 차원 축소 part 를 줄여야 함 -> 파일을 새로 저장해야할듯!\n",
        "    X = title_vect2\n",
        "    svd = TruncatedSVD(n_components=500, n_iter=7, random_state=42)\n",
        "    X_new=svd.fit_transform(X)\n",
        "\n",
        "    D = X_new.shape[1]\n",
        "    K = 500  # The number of clusters\n",
        "    X = X_new.astype(np.float32)\n",
        "\n",
        "    # Setup\n",
        "    kmeans = faiss.Kmeans(d=D, k=K, niter=20, verbose=True,gpu=True)\n",
        "    # For GPU(s), run the following line. This will use all GPUs\n",
        "    # kmeans = faiss.Kmeans(d=D, k=K, niter=20, verbose=True, gpu=True)\n",
        "\n",
        "    # Run clustering\n",
        "    kmeans.train(X)\n",
        "\n",
        "    # Error for each iteration\n",
        "    # print(kmeans.obj)  # array with 20 elements\n",
        "\n",
        "    # Centroids after clustering\n",
        "    # print(kmeans.centroids.shape)  # (10, 500)\n",
        "\n",
        "    # The assignment for each vector.\n",
        "    dists, ids = kmeans.index.search(X, 1)  # Need to run NN search again\n",
        "    books_df_nax['tit_cluster']=ids\n",
        "    booksdf=books_df_nax[['isbn13','title','description','tit_cluster']]\n",
        "\n",
        "    def userbooks(A):\n",
        "        l=len(A)\n",
        "        n=random.randrange(0,l)\n",
        "        return A[n]\n",
        "\n",
        "    # 사용자의 isbn 데이터를 담은 책 리스트를 input으로 받아 그 리스트 중 하나를 반환하는 함수\n",
        "    # B = 사용자 데이터에서 랜덤 추출한 책의 isbn \n",
        "    B=userbooks(U)\n",
        "\n",
        "\n",
        "    def sim_idx_with_A(df,isbn):\n",
        "        n=int(df[df['isbn13']==isbn]['tit_cluster'])\n",
        "        same_clu_books_df=df[df['tit_cluster']==n]\n",
        "        sen = same_clu_books_df['description']\n",
        "\n",
        "        # 1번 후보 : sentence transformer 유사도 구하기\n",
        "        #from sentence_transformers import SentenceTransformer\n",
        "        #model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        #sen_embeddings = model.encode(sen)\n",
        "    \n",
        "        # 2번 후보 : 단순 tfidf 활용한 유사도 구하기\n",
        "        clu_tfidf = tfidf.fit_transform(sen)\n",
        "        clu_des_sim=cosine_similarity(clu_tfidf,clu_tfidf)\n",
        "\n",
        "        # idx_list=same_clu_books_df.index[same_clu_books_df['title']==books].tolist()\n",
        "        idx=np.where(same_clu_books_df['isbn13']==B)[0][0]\n",
        "        books_sim_vect=clu_des_sim[idx:idx+1]\n",
        "        books_des_sim_idx = books_sim_vect.argsort()[::-1]\n",
        "\n",
        "        return books_des_sim_idx\n",
        "\n",
        "    # 사용자 책 리스트에서 랜덤 추출한 책 기준 유사한 책 인덱스 리스트 반환\n",
        "    sim_books_idx=sim_idx_with_A(books_df_nax,B)\n",
        "\n",
        "    #books=userbooks(A)\n",
        "    #n=int(df[df['title']==books]['tit_cluster'])\n",
        "    #same_clu_books_df=df[df['tit_cluster']==n]\n",
        "    #sen = same_clu_books_df['description']\n",
        "    top_n=5\n",
        "    top_sim_idx=sim_books_idx[0][:top_n]\n",
        "    top_sim_idx=top_sim_idx.reshape(-1,)\n",
        "    sim_books=books_df_nax.iloc[top_sim_idx]\n",
        "    outputs=sim_books[['isbn13','title','img_url']]\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "7ioOLOyEvDa8"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYCJSW5wHAWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGCK3YhuN69X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDgP6LkON6_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hB210p9AN7CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0FkjDm0N7E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9J3mvUdXN7H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgVeJvxXOAVg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}